{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import models\n",
    "import random\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"seed\": 42,\n",
    "\"n_split\": 5,\n",
    "\"n_epoch\": 5,\n",
    "\"batch_size\": 8,\n",
    "\"att_num\": 4,                                               # Number of current attempt\n",
    "\"weights_att_num\": None,  # Number of attempt for pre-define model\n",
    "\"model_name\": \"padded_with_smoothing\",                                                 # ML model name\n",
    "\"opt_name\": \"Adam\",                                                   # Optimizer (custom filling opt_name)\n",
    "\"lr\": 0.00003,                                                  # Initial LR\n",
    "\"lr_schedule_name\": \"default\",                                          # LR checduling (custom filling opt_name)\n",
    "\"n_fold\": 1,                                               # fold for training\n",
    "\"start_epoch\": 1,                                          # start epoch for training\n",
    "\"wo_fitting\": False,\n",
    "\"label_consider_type\": \"left\",                                       # way to using positive/nutral/negative label\n",
    "\"label_smoothing\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/tweet-sentiment-extraction/train.csv does not exist: '../input/tweet-sentiment-extraction/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-98c8f2a8c69f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-98c8f2a8c69f>\u001b[0m in \u001b[0;36mread_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/tweet-sentiment-extraction/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/tweet-sentiment-extraction/train.csv does not exist: '../input/tweet-sentiment-extraction/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import models\n",
    "import random\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "PAD_ID = 1\n",
    "\n",
    "# def fold_processing(params):\n",
    "\n",
    "################################### INPUT DATA ###################################\n",
    "\n",
    "def read_train():\n",
    "    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "    train['text']=train['text'].astype(str)\n",
    "    train['selected_text']=train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "def read_test():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "    test['text']=test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "def read_submission():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "    return test\n",
    "\n",
    "train_df = read_train()\n",
    "test_df = read_test()\n",
    "submission_df = read_submission()\n",
    "\n",
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "\n",
    "################################### TRAIN DATA ###################################\n",
    "\n",
    "N_TRAIN = train_df.shape[0]\n",
    "# args from https://huggingface.co/transformers/model_doc/roberta.html?highlight=tfrobertamodel#tfrobertamodel\n",
    "input_ids = np.ones((N_TRAIN, MAX_LEN), dtype='int32')       # token ids (pre-trained vocabulary & tokenizer)\n",
    "attention_mask = np.zeros((N_TRAIN, MAX_LEN), dtype='int32') # 0 in padding\n",
    "token_type_ids = np.zeros((N_TRAIN, MAX_LEN), dtype='int32') # 0 for A sentence, 1 - for B (there is only A sentence)\n",
    "\n",
    "start_tokens = np.zeros((N_TRAIN, MAX_LEN), dtype='int32')\n",
    "end_tokens = np.zeros((N_TRAIN, MAX_LEN), dtype='int32')\n",
    "\n",
    "for k in range(N_TRAIN):\n",
    "\n",
    "    # FIND OVERLAP (mask with 1 including first whitespace)\n",
    "    text1 = \" \" + \" \".join(train_df.loc[k, 'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k, 'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx - 1] == ' ': \n",
    "        chars[idx - 1] = 1\n",
    "\n",
    "    # ID_OFFSETS (offsets = [(start1, finish1), .., (startN, finishN)])\n",
    "    enc = tokenizer.encode(text1) \n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx, idx + len(w)))\n",
    "        idx += len(w)\n",
    "\n",
    "    # START END TOKENS (toks - list of tokens from selected text)\n",
    "    toks = []\n",
    "    for i, (a,b) in enumerate(offsets):\n",
    "        if np.sum(chars[a:b]) > 0:\n",
    "            toks.append(i) \n",
    "\n",
    "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "#         input_ids[k,:len(enc.ids) + 5] = [0] + enc.ids + [2, 2] + [s_tok] + [2]\n",
    "#         attention_mask[k,:len(enc.ids) + 5] = 1\n",
    "    if params[\"label_consider_type\"] == \"right\":\n",
    "        LEFT_PAD_LEN = 1\n",
    "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    elif params[\"label_consider_type\"] == \"left\":\n",
    "        LEFT_PAD_LEN = 2\n",
    "        input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "        attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    else:\n",
    "        assert False, \"unknown label_consider_type param\"\n",
    "\n",
    "    if len(toks) > 0:\n",
    "        start_tokens[k, toks[0] + 1] = 1\n",
    "        end_tokens[k, toks[-1] + 1] = 1\n",
    "\n",
    "################################### TEST DATA ###################################\n",
    "\n",
    "N_TEST = test_df.shape[0]\n",
    "test_word_ids = np.ones((N_TEST, MAX_LEN),dtype='int32')\n",
    "test_mask = np.zeros((N_TEST, MAX_LEN),dtype='int32')\n",
    "test_segm_ids = np.zeros((N_TEST, MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(N_TEST):\n",
    "\n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
    "    test_word_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    test_mask[k,:len(enc.ids)+5] = 1\n",
    "    if params[\"label_consider_type\"] == \"right\":\n",
    "        LEFT_PAD_LEN = 1\n",
    "        test_word_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        test_mask[k,:len(enc.ids)+5] = 1\n",
    "    elif params[\"label_consider_type\"] == \"left\":\n",
    "        LEFT_PAD_LEN = 2\n",
    "        test_word_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "        test_mask[k,:len(enc.ids)+3] = 1\n",
    "    else:\n",
    "        assert False, \"unknown label_consider_type param\"\n",
    "\n",
    "\n",
    "################################### POSTPROCESS FUNCTIONS ###################################\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str(str1).lower().split()) \n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_pred(start_proba, end_proba, df, tokenizer, out_prefix):\n",
    "    pred = []\n",
    "    n_samples = len(start_proba)\n",
    "    for i in range(n_samples):\n",
    "        text = df['text'][df.index[i]]\n",
    "        a, b = np.argmax(start_proba[i,]), np.argmax(end_proba[i,])\n",
    "        if a > b: \n",
    "            pred_ = text # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            cleaned_text = \" \" + \" \".join(text.split())\n",
    "            encoded_text = tokenizer.encode(cleaned_text)\n",
    "            pred_ids = encoded_text.ids[a - LEFT_PAD_LEN: b - LEFT_PAD_LEN + 1]\n",
    "            pred_ = tokenizer.decode(pred_ids)\n",
    "        pred += [pred_]\n",
    "\n",
    "    if out_prefix:\n",
    "        DataFrame(start_proba).to_csv(\"{}_start_prediction.csv\".format(out_prefix))\n",
    "        DataFrame(end_proba  ).to_csv(\"{}_end_prediction.csv\"  .format(out_prefix))\n",
    "        df_pred = df.copy()\n",
    "        df_pred[\"pred_selected_text\"] = pred\n",
    "        df_pred.to_csv(\"{}_prediction.csv\".format(out_prefix))\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_metric(trues, preds):\n",
    "\n",
    "    return np.mean([\n",
    "        jaccard(pred, true)\n",
    "        for true, pred in zip(trues, preds)\n",
    "    ])\n",
    "\n",
    "def get_pred_and_metric(start_proba, end_proba, df, tokenizer, out_prefix=None):\n",
    "    pred = get_pred(start_proba, end_proba, df, tokenizer, out_prefix)\n",
    "    metric = None\n",
    "    if 'selected_text' in df:\n",
    "        true = df['selected_text']\n",
    "        metric = get_metric(true, pred)\n",
    "    return pred, metric\n",
    "\n",
    "################################### MODEL ###################################\n",
    "\n",
    "def build_model(opt):\n",
    "    ids = Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    # https://huggingface.co/transformers/model_doc/roberta.html?highlight=tfrobertamodel#tfrobertamodel\n",
    "    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    if not params[\"label_smoothing\"]:\n",
    "        params[\"label_smoothing\"] = 0.\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=params[\"label_smoothing\"])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def build_model_with_smoothing(opt):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_padded_model(model):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "\n",
    "    x = model([ids, att, tok])\n",
    "    x1_padded = tf.pad(x[0], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x[1], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "\n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded, x2_padded])\n",
    "    return padded_model\n",
    "\n",
    "\n",
    "\n",
    "################################### CALLBACKS ###################################\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, model, word_ids, mask, segm_ids, start, end, df, tokenizer, n_fold, start_epoch, log_path, start_score, best_weights_path):\n",
    "        self.model = model\n",
    "\n",
    "        self.word_ids = word_ids\n",
    "        self.mask = mask\n",
    "        self.segm_ids = segm_ids\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.n_fold = n_fold\n",
    "        self.log_path = log_path\n",
    "        self.best_weights_path = best_weights_path\n",
    "\n",
    "        self.best = start_score\n",
    "        self.checkpoint = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Validation\n",
    "        padded_model = build_padded_model(self.model)\n",
    "        start_proba, end_proba = tuple(padded_model.predict([self.word_ids, self.mask, self.segm_ids], verbose=1))\n",
    "        _, current = get_pred_and_metric(start_proba, end_proba, self.df, self.tokenizer)\n",
    "\n",
    "        # Save best model\n",
    "        if current > self.best:\n",
    "            self.best = current\n",
    "            self.model.save_weights(self.best_weights_path, overwrite=True)\n",
    "\n",
    "        # Log score info\n",
    "        abs_epoch = self.start_epoch + epoch\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(f'\\n[fold: {self.n_fold}, epoch: {abs_epoch}] Val Score : {current:.5f} (time: {(time.time() - self.checkpoint) // 60} min.)')\n",
    "        self.checkpoint = time.time()\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "\n",
    "    return 3e-5 * 0.2**epoch\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "################################### SEED & SESSION ###################################\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(params[\"seed\"])\n",
    "#     K.clear_session()\n",
    "#     config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=4)\n",
    "#     sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "#     tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "################################### ARGS ###################################\n",
    "\n",
    "if params[\"opt_name\"] == \"Adam\":\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "else:\n",
    "    assert False, \"unknown opt_name\"\n",
    "\n",
    "if params[\"lr_schedule_name\"] == \"default\":\n",
    "    lr_scheduler = lr_scheduler\n",
    "else:\n",
    "    assert False, \"unknown lr_schedule_name\"\n",
    "\n",
    "################################### LOGGING ###################################\n",
    "\n",
    "# define log-folder\n",
    "log_dir_path = \"../attempt_logs/{}\".format(params[\"att_num\"])\n",
    "if not os.path.exists(log_dir_path):\n",
    "    os.makedirs(log_dir_path)\n",
    "\n",
    "# define fold-folders log_path\n",
    "folder_path = \"{}/{}/\".format(log_dir_path, params[\"n_fold\"])\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# define & prefill log-file\n",
    "log_path = \"{}/log.txt\".format(log_dir_path, params[\"att_num\"])\n",
    "if not os.path.exists(log_path):\n",
    "    with open(log_path, 'w') as f:\n",
    "        for name, val in params.items():\n",
    "            f.write(\"{}: {} \\n\".format(name, val))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "################################### OPTIMIZATION PROCESSING ###################################\n",
    "\n",
    "# Fold Splitter\n",
    "skf = StratifiedKFold(n_splits=params[\"n_split\"], shuffle=True, random_state=777)\n",
    "splits = list(skf.split(input_ids, train_df.sentiment.values))\n",
    "\n",
    "# Splitting\n",
    "tr_idx, val_idx = splits[params[\"n_fold\"] - 1]\n",
    "\n",
    "train_df_ = train_df.loc[tr_idx].reset_index(drop=True)\n",
    "train_df_.index = tr_idx\n",
    "\n",
    "val_df = train_df.loc[val_idx].reset_index(drop=True)\n",
    "val_df.index = val_idx\n",
    "\n",
    "tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = input_ids[tr_idx,], attention_mask[tr_idx,], token_type_ids[tr_idx,], start_tokens[tr_idx,], end_tokens[tr_idx,]\n",
    "val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = input_ids[val_idx,], attention_mask[val_idx,], token_type_ids[val_idx,], start_tokens[val_idx,], end_tokens[val_idx,]\n",
    "\n",
    "\n",
    "N_TEST = 16\n",
    "tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = tr_word_ids[16:32], tr_mask[16:32], tr_segm_ids[16:32], tr_starts[16:32], tr_ends[16:32]\n",
    "val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = val_word_ids[16:32], val_mask[16:32], val_segm_ids[16:32], val_starts[16:32], val_ends[16:32]\n",
    "tr_idx = tr_idx[16:32]\n",
    "val_idx = val_idx[16:32]\n",
    "train_df_ = train_df_[16:32]\n",
    "val_df = val_df[16:32]\n",
    "test_word_ids, test_mask, test_segm_ids = test_word_ids[16:32], test_mask[16:32], test_segm_ids[16:32]\n",
    "test_df = test_dff[16:32]\n",
    "\n",
    "\n",
    "print(f'##### FOLD {params[\"n_fold\"]} #####')\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Model Defining\n",
    "best_weights_path = \"{}/{}/best_model.h5\".format(log_dir_path, params[\"n_fold\"])\n",
    "if params[\"model_name\"] == \"default\":\n",
    "    model = build_model(opt)\n",
    "elif params[\"model_name\"] == \"padded_with_smoothing\":\n",
    "    model = build_model_with_smoothing(opt)\n",
    "else:\n",
    "    assert False, \"unknown model_name param\"\n",
    "\n",
    "\n",
    "# Model Pretraining [optional]\n",
    "pre_trained_weights_path = \"../attempt_logs/{}/{}/best_model.h5\".format(\n",
    "    params[\"weights_att_num\"] or params[\"att_num\"],\n",
    "    params[\"n_fold\"]\n",
    ")\n",
    "pretrained_score = 0\n",
    "if os.path.exists(pre_trained_weights_path):\n",
    "    model.load_weights(pre_trained_weights_path)\n",
    "    padded_model = build_padded_model(model)\n",
    "    start_proba, end_proba = tuple(padded_model.predict([val_word_ids, val_mask, val_segm_ids], verbose=1))\n",
    "    _, pretrained_score = get_pred_and_metric(start_proba, end_proba, val_df, tokenizer)\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(f'\\nWeights PreTrained from {pre_trained_weights_path}, pretrained_score: {pretrained_score:.5f}')\n",
    "\n",
    "# Model Pretraining [optional, when we need only ensure total scores]\n",
    "if not params[\"wo_fitting\"]:\n",
    "    custom_callback = CustomCallback(\n",
    "        model,\n",
    "        val_word_ids, val_mask, val_segm_ids, val_starts, val_ends, val_df,\n",
    "        tokenizer,\n",
    "        params[\"n_fold\"],\n",
    "        params[\"start_epoch\"],\n",
    "        log_path,\n",
    "        pretrained_score,\n",
    "        best_weights_path\n",
    "    )\n",
    "\n",
    "    n_remain_epoch = params[\"n_epoch\"] - params[\"start_epoch\"] + 1\n",
    "    model.fit(\n",
    "        [tr_word_ids, tr_mask, tr_segm_ids], [tr_starts, tr_ends],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        epochs=n_remain_epoch,\n",
    "        callbacks=[\n",
    "            custom_callback,\n",
    "            lr_scheduler\n",
    "        ],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "model.load_weights(best_weights_path)\n",
    "padded_model = build_padded_model(model)\n",
    "\n",
    "\n",
    "# train prediction\n",
    "print(\"train prediction ...\")\n",
    "train_start_proba, train_end_proba = tuple(padded_model.predict([tr_word_ids, tr_mask, tr_segm_ids], verbose=1))\n",
    "_, train_metric = get_pred_and_metric(train_start_proba, train_end_proba, train_df_, tokenizer, out_prefix=\"{}/{}/train\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "# validation prediction\n",
    "print(\"validation prediction ...\")\n",
    "val_start_proba, val_end_proba = tuple(padded_model.predict([val_word_ids, val_mask, val_segm_ids], verbose=1))\n",
    "_, val_metric = get_pred_and_metric(val_start_proba, val_end_proba, val_df, tokenizer, out_prefix=\"{}/{}/validation\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "# test prediction\n",
    "print(\"test prediction ...\")\n",
    "test_start_proba_, test_end_proba_ = tuple(padded_model.predict([test_word_ids, test_mask, test_segm_ids], verbose=1))\n",
    "_ = get_pred_and_metric(test_start_proba_, test_end_proba_, test_df, tokenizer, out_prefix=\"{}/{}/test\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f'\\n[fold: {params[\"n_fold\"]}] Ensure Scores : train score: {train_metric:.5f}, validation score: {val_metric:.5f}]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fold_processing(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "n_folds=5\n",
    "\n",
    "n_epochs=5\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "# Number of current attempt\n",
    "att_num=4\n",
    "\n",
    "# Number of attempt for pre-define model\n",
    "weights_att_num=None\n",
    "\n",
    "# Model (custom filling model_name) [default, padded_with_smoothing]\n",
    "model_name=padded_with_smoothing\n",
    "\n",
    "# Optimizer (custom filling opt_name)\n",
    "opt_name=Adam\n",
    "\n",
    "# LR scheduling (custom filling opt_name)\n",
    "lr=0.00003\n",
    "\n",
    "# LR checduling (custom filling opt_name)\n",
    "lr_schedule_name=default\n",
    "\n",
    "# Way to using positive/nutral/negative label [left, right]\n",
    "label_consider_type=left\n",
    "\n",
    "# Label smoothing for model_name=padded_with_smoothing\n",
    "label_smoothing=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
