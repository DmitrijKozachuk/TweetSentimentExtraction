{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from constants import *\n",
    "from callback import CustomCallback\n",
    "from base_model import get_base_model\n",
    "from head_model import get_combined_model\n",
    "from loss import get_loss\n",
    "from data_reading import read_data\n",
    "from data_preparation import get_train_data, get_test_data\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14184195797816832121\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15303945521753371595\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5254660559198242221\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3868065792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1964583407835723230\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_path\": \"..\",\n",
    "    \"mode\": \"debug:1600\",\n",
    "    \"att_num\": 68,\n",
    "    \"weights_att_num\": None,\n",
    "    \"n_fold\": 1,\n",
    "    \"start_epoch\": 4,\n",
    "    \"wo_fitting\": False,\n",
    "\n",
    "    \"base_model\": \"roberta-base\",\n",
    "    \"head_model\": \"default\",\n",
    "    \"lr\": 0.00003, # \"base_lr\": 0.00002, \"max_lr\": 0.00004,\n",
    "    \"loss\": \"CCE\", #\"{'CCE':1, 'JEL':0.1}\",\n",
    "    \"label_smoothing\": 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data...\n",
      "Build & Compile model...\n",
      "roberta-base\n",
      "Prepare data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0c710e2399492eb87e8ef197f12295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b24a4065c144167b14332d053cb7500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data...\n",
      "Check Correcness...\n",
      "correct samples: 0.890625\n",
      "preprocessing OK!\n",
      "##### FOLD 1 #####\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "50/50 [==============================] - 19s 374ms/step\n",
      "200/200 [==============================] - 103s 515ms/step - loss: 5.2134 - activation_7_loss: 2.5835 - activation_8_loss: 2.6299 - lr: 3.0000e-05\n",
      "Epoch 2/2\n",
      " 50/200 [======>.......................] - ETA: 49s - loss: 4.6867 - activation_7_loss: 2.3554 - activation_8_loss: 2.3313"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-77a435f3feee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         ],\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Seed $ Logging\n",
    "seed_everything(SEED)\n",
    "log_dir_path, log_path = init_logging(params)\n",
    "\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f'\\n[base_model] {params[\"base_model\"]}')\n",
    "\n",
    "# Read  data\n",
    "print(\"Read data...\")\n",
    "train_df, test_df, submission_df = read_data(params)\n",
    "\n",
    "# Splitter\n",
    "skf = StratifiedKFold(n_splits=N_SPLIT, shuffle=True, random_state=777)\n",
    "splits = list(skf.split(train_df.index.values, train_df.sentiment.values))\n",
    "tr_idx, val_idx = splits[params[\"n_fold\"] - 1]\n",
    "test_idx = np.arange(N_TEST)\n",
    "\n",
    "if \"debug\" in params[\"mode\"]:\n",
    "    n_debug = int(params[\"mode\"].split(\":\")[1])\n",
    "    tr_idx, val_idx = tr_idx[:n_debug], val_idx[:n_debug]\n",
    "    test_idx = test_idx[:n_debug]\n",
    "\n",
    "# Build & Compile model\n",
    "print(\"Build & Compile model...\")\n",
    "tokenizer, base_model = get_base_model(params)\n",
    "combined_model = get_combined_model(base_model, params)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "loss = get_loss(params)\n",
    "combined_model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "# Prepare  data\n",
    "print(\"Prepare data...\")\n",
    "known_idx = np.array(list(set(tr_idx) | set(val_idx)))\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens, train_sample_ind2new_ind2old_ind = get_train_data(train_df, tokenizer, idx=known_idx)\n",
    "test_word_ids, test_mask, test_segm_ids, test_sample_ind2new_ind2old_ind = get_test_data(test_df, tokenizer, idx=test_idx)\n",
    "\n",
    "# # Model hash\n",
    "# print(f'base_model hash: {np.array(base_model(test_word_ids[:16], test_mask[:16], test_segm_ids[:16])[0]).sum():.3}')\n",
    "# print(f'head_model hash: {combined_model.layers[-6].weights[0].numpy().sum():.3}')\n",
    "\n",
    "# Splitting data\n",
    "print(\"Splitting data...\")\n",
    "tr_df = train_df.loc[tr_idx].reset_index(drop=True).set_index(tr_idx)\n",
    "val_df = train_df.loc[val_idx].reset_index(drop=True).set_index(val_idx)\n",
    "\n",
    "tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = input_ids[tr_idx,], attention_mask[tr_idx,], token_type_ids[tr_idx,], start_tokens[tr_idx,], end_tokens[tr_idx,]\n",
    "tr_targets = np.concatenate([tr_starts, tr_ends], axis=1)\n",
    "val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = input_ids[val_idx,], attention_mask[val_idx,], token_type_ids[val_idx,], start_tokens[val_idx,], end_tokens[val_idx,]\n",
    "\n",
    "# Check Correcness\n",
    "print(\"Check Correcness...\")\n",
    "tr_df[\"is_correct\"] = tr_df.apply(lambda row: (\" \" + row.text + \" \").find(\" \" + row.selected_text + \" \") >= 0, axis=1)\n",
    "print(f'correct samples: {tr_df[\"is_correct\"].mean():3f}')\n",
    "\n",
    "tr_df[\"recover_selected_text\"] = get_st_prediction(tr_starts, tr_ends, tr_df, train_sample_ind2new_ind2old_ind)\n",
    "tr_df[\"recover_jaccard\"] = tr_df.apply(lambda row: jaccard(row[\"recover_selected_text\"], row[\"selected_text\"]), axis=1)\n",
    "assert np.all(tr_df[tr_df[\"is_correct\"]][\"recover_jaccard\"] == 1)\n",
    "print(f'preprocessing OK!')\n",
    "\n",
    "\n",
    "print(f'##### FOLD {params[\"n_fold\"]} #####')\n",
    "gc.collect()\n",
    "\n",
    "# Model Paths & Pretraining (optional)\n",
    "best_weights_path = f'{log_dir_path}/{params[\"n_fold\"]}/best_model.h5'\n",
    "pre_trained_weights_path = f'../attempt_logs/{params[\"weights_att_num\"] or params[\"att_num\"]}/{params[\"n_fold\"]}/best_model.h5'\n",
    "\n",
    "pretrained_score = 0\n",
    "# if os.path.exists(pre_trained_weights_path):\n",
    "#     combined_model.load_weights(pre_trained_weights_path)\n",
    "#     start_proba, end_proba = get_proba_prediction(combined_model, val_word_ids, val_mask, val_segm_ids)\n",
    "#     pretrained_score = get_score(start_proba, end_proba, val_df, train_sample_ind2new_ind2old_ind)\n",
    "#     with open(log_path, 'a') as f:\n",
    "#         f.write(f'\\nWeights PreTrained from {pre_trained_weights_path}, pretrained_score: {pretrained_score:.5f}')\n",
    "\n",
    "# Training (optional)\n",
    "if not params[\"wo_fitting\"]:\n",
    "    lr_scheduler = LearningRateScheduler(lambda epoch: 3e-5 * 0.2**epoch)\n",
    "    custom_callback = CustomCallback(\n",
    "        combined_model,\n",
    "        val_word_ids, val_mask, val_segm_ids, val_df, train_sample_ind2new_ind2old_ind,\n",
    "        params[\"n_fold\"],\n",
    "        params[\"start_epoch\"],\n",
    "        log_path,\n",
    "        pretrained_score,\n",
    "        best_weights_path\n",
    "    )\n",
    "\n",
    "    n_epoch = N_EPOCH - params[\"start_epoch\"] + 1\n",
    "    combined_model.fit(\n",
    "        [tr_word_ids, tr_mask, tr_segm_ids], [tr_starts, tr_ends], #tr_targets,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=n_epoch,\n",
    "        callbacks=[\n",
    "            custom_callback,\n",
    "            lr_scheduler\n",
    "        ],\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "combined_model.load_weights(best_weights_path)\n",
    "\n",
    "# scores = {}\n",
    "# for name, word_ids, mask, segm_ids, df, sample_ind2new_ind2old_ind in [\n",
    "#     (\"train\"      ,  tr_word_ids,   tr_mask,   tr_segm_ids,   tr_df, train_sample_ind2new_ind2old_ind),\n",
    "#     (\"validation\" , val_word_ids,  val_mask,  val_segm_ids,  val_df, train_sample_ind2new_ind2old_ind),\n",
    "#     (\"test\"      , test_word_ids, test_mask, test_segm_ids, test_df,  test_sample_ind2new_ind2old_ind)\n",
    "# ]:\n",
    "#     print(f'{name} prediction ...')\n",
    "#     start_proba, end_proba = get_proba_prediction(combined_model, word_ids, mask, segm_ids)\n",
    "#     if name != \"test\":\n",
    "#         scores[name] = get_score(start_proba, end_proba, df, sample_ind2new_ind2old_ind)\n",
    "\n",
    "# with open(log_path, 'a') as f:\n",
    "#     f.write(f'\\n[fold: {params[\"n_fold\"]}] Ensure Scores : train score: {scores[\"train\"]:.5f}, validation score: {scores[\"validation\"]:.5f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tr_word_ids[0] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tr_word_ids[1] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_starts[::, 50::].sum(), tr_ends[::, 50::].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1600, 0), dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_starts[::, 113:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import models\n",
    "tf.compat.v1.disable_eager_execution() # for tf placeholders\n",
    "\n",
    "def smoothed_cce_loss(y_true, y_pred):\n",
    "    print(\"y_true\", y_true)\n",
    "    print(\"y_pred\", y_pred)\n",
    "#     ls = params[\"label_smoothing\"] or 0\n",
    "    ls = 0.1\n",
    "\n",
    "#     max_len = K.shape(y_pred)[1] // 2\n",
    "    max_len = 100\n",
    "    start_pred, end_pred = y_pred[:, :max_len], y_pred[:, MAX_LEN:MAX_LEN + max_len]\n",
    "    start_true, end_true = y_true[:, :max_len], y_true[:, MAX_LEN:MAX_LEN + max_len]\n",
    "    print(\"start_pred\", start_pred)\n",
    "    print(\"end_pred\", end_pred)\n",
    "    print(\"start_true\", start_true)\n",
    "    print(\"end_true\", end_true)\n",
    "    \n",
    "\n",
    "    start_loss = tf.keras.losses.categorical_crossentropy(start_true, start_pred, label_smoothing=ls)\n",
    "    end_loss =   tf.keras.losses.categorical_crossentropy(  end_true,   end_pred, label_smoothing=ls)\n",
    "    loss = tf.reduce_mean(start_loss + end_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 226), (8, 226))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 113\n",
    "\n",
    "y_true = tr_targets[:8]\n",
    "y_pred = tr_targets[:8]\n",
    "\n",
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 13), (8, 13))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 100\n",
    "\n",
    "assert y_pred[::, max_len:MAX_LEN].sum() + y_pred[::, MAX_LEN + max_len:].sum() == 0\n",
    "y_pred[::, max_len:MAX_LEN].shape, y_pred[::, MAX_LEN + max_len:].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true Tensor(\"Placeholder_26:0\", shape=(None, 226), dtype=float32)\n",
      "y_pred Tensor(\"Placeholder_27:0\", shape=(None, 226), dtype=float32)\n",
      "start_pred Tensor(\"strided_slice_84:0\", shape=(None, 100), dtype=float32)\n",
      "end_pred Tensor(\"strided_slice_85:0\", shape=(None, 100), dtype=float32)\n",
      "start_true Tensor(\"strided_slice_86:0\", shape=(None, 100), dtype=float32)\n",
      "end_true Tensor(\"strided_slice_87:0\", shape=(None, 100), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1913834"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 226])\n",
    "y_true_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 226])\n",
    "\n",
    "cce = smoothed_cce_loss(y_pred_inp, y_true_inp)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "jel = sess.run(cce, feed_dict={y_pred_inp: y_pred, y_true_inp: y_true})\n",
    "jel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 226)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model hash: 2.49e+04\n",
    "head_model hash: -0.233"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
