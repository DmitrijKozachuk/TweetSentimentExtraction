{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import models\n",
    "import random\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"seed\": 42,\n",
    "\"n_split\": 5,\n",
    "\"n_epoch\": 5,\n",
    "\"batch_size\": 8,\n",
    "\"att_num\": 10,                                               # Number of current attempt\n",
    "\"weights_att_num\": None,  # Number of attempt for pre-define model\n",
    "\"model_name\": \"default\",            #padded_with_smoothing                                      # ML model name\n",
    "\"opt_name\": \"Adam\",                                                   # Optimizer (custom filling opt_name)\n",
    "\"lr\": 0.00003,                                                  # Initial LR\n",
    "\"lr_schedule_name\": \"default\",                                          # LR checduling (custom filling opt_name)\n",
    "\"n_fold\": 1,                                               # fold for training\n",
    "\"start_epoch\": 1,                                          # start epoch for training\n",
    "\"wo_fitting\": False,\n",
    "\"label_consider_type\": \"left\",                                       # way to using positive/nutral/negative label\n",
    "\"label_smoothing\": 0.1\n",
    "}\n",
    "\n",
    "PAD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### FOLD 1 #####\n"
     ]
    }
   ],
   "source": [
    "################################### INPUT DATA ###################################\n",
    "\n",
    "def read_train():\n",
    "    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "    train['text']=train['text'].astype(str)\n",
    "    train['selected_text']=train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "def read_test():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "    test['text']=test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "def read_submission():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "    return test\n",
    "\n",
    "train_df = read_train()\n",
    "test_df = read_test()\n",
    "submission_df = read_submission()\n",
    "\n",
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "\n",
    "################################### TRAIN DATA ###################################\n",
    "\n",
    "N_TRAIN = train_df.shape[0]\n",
    "# args from https://huggingface.co/transformers/model_doc/roberta.html?highlight=tfrobertamodel#tfrobertamodel\n",
    "input_ids = np.ones((N_TRAIN, MAX_LEN), dtype='int32')       # token ids (pre-trained vocabulary & tokenizer)\n",
    "attention_mask = np.zeros((N_TRAIN, MAX_LEN), dtype='int32') # 0 in padding\n",
    "token_type_ids = np.zeros((N_TRAIN, MAX_LEN), dtype='int32') # 0 for A sentence, 1 - for B (there is only A sentence)\n",
    "\n",
    "start_tokens = np.zeros((N_TRAIN, MAX_LEN), dtype='int32')\n",
    "end_tokens = np.zeros((N_TRAIN, MAX_LEN), dtype='int32')\n",
    "\n",
    "for k in range(N_TRAIN):\n",
    "\n",
    "    # FIND OVERLAP (mask with 1 including first whitespace)\n",
    "    text1 = \" \" + \" \".join(train_df.loc[k, 'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k, 'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx - 1] == ' ': \n",
    "        chars[idx - 1] = 1\n",
    "\n",
    "    # ID_OFFSETS (offsets = [(start1, finish1), .., (startN, finishN)])\n",
    "    enc = tokenizer.encode(text1) \n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx, idx + len(w)))\n",
    "        idx += len(w)\n",
    "\n",
    "    # START END TOKENS (toks - list of tokens from selected text)\n",
    "    toks = []\n",
    "    for i, (a,b) in enumerate(offsets):\n",
    "        if np.sum(chars[a:b]) > 0:\n",
    "            toks.append(i) \n",
    "\n",
    "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "#         input_ids[k,:len(enc.ids) + 5] = [0] + enc.ids + [2, 2] + [s_tok] + [2]\n",
    "#         attention_mask[k,:len(enc.ids) + 5] = 1\n",
    "    if params[\"label_consider_type\"] == \"right\":\n",
    "        LEFT_PAD_LEN = 1\n",
    "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    elif params[\"label_consider_type\"] == \"left\":\n",
    "        LEFT_PAD_LEN = 2\n",
    "        input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "        attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    else:\n",
    "        assert False, \"unknown label_consider_type param\"\n",
    "\n",
    "    if len(toks) > 0:\n",
    "        start_tokens[k, toks[0] + LEFT_PAD_LEN] = 1\n",
    "        end_tokens[k, toks[-1] + LEFT_PAD_LEN] = 1\n",
    "\n",
    "################################### TEST DATA ###################################\n",
    "\n",
    "N_TEST = test_df.shape[0]\n",
    "test_word_ids = np.ones((N_TEST, MAX_LEN),dtype='int32')\n",
    "test_mask = np.zeros((N_TEST, MAX_LEN),dtype='int32')\n",
    "test_segm_ids = np.zeros((N_TEST, MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(N_TEST):\n",
    "\n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
    "    test_word_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    test_mask[k,:len(enc.ids)+5] = 1\n",
    "    if params[\"label_consider_type\"] == \"right\":\n",
    "        LEFT_PAD_LEN = 1\n",
    "        test_word_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        test_mask[k,:len(enc.ids)+5] = 1\n",
    "    elif params[\"label_consider_type\"] == \"left\":\n",
    "        LEFT_PAD_LEN = 2\n",
    "        test_word_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "        test_mask[k,:len(enc.ids)+3] = 1\n",
    "    else:\n",
    "        assert False, \"unknown label_consider_type param\"\n",
    "\n",
    "\n",
    "################################### POSTPROCESS FUNCTIONS ###################################\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str(str1).lower().split()) \n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_pred(start_proba, end_proba, df, tokenizer, out_prefix):\n",
    "    pred = []\n",
    "    n_samples = len(start_proba)\n",
    "    for i in range(n_samples):\n",
    "        text = df['text'][df.index[i]]\n",
    "        a, b = np.argmax(start_proba[i,]), np.argmax(end_proba[i,])\n",
    "        if a > b: \n",
    "            pred_ = text # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            cleaned_text = \" \" + \" \".join(text.split())\n",
    "            encoded_text = tokenizer.encode(cleaned_text)\n",
    "            pred_ids = encoded_text.ids[a - LEFT_PAD_LEN: b - LEFT_PAD_LEN + 1]\n",
    "            pred_ = tokenizer.decode(pred_ids)\n",
    "        pred += [pred_]\n",
    "\n",
    "    if out_prefix:\n",
    "        DataFrame(start_proba).to_csv(\"{}_start_prediction.csv\".format(out_prefix))\n",
    "        DataFrame(end_proba  ).to_csv(\"{}_end_prediction.csv\"  .format(out_prefix))\n",
    "        df_pred = df.copy()\n",
    "        df_pred[\"pred_selected_text\"] = pred\n",
    "        df_pred.to_csv(\"{}_prediction.csv\".format(out_prefix))\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_metric(trues, preds):\n",
    "\n",
    "    return np.mean([\n",
    "        jaccard(pred, true)\n",
    "        for true, pred in zip(trues, preds)\n",
    "    ])\n",
    "\n",
    "def get_pred_and_metric(start_proba, end_proba, df, tokenizer, out_prefix=None):\n",
    "    pred = get_pred(start_proba, end_proba, df, tokenizer, out_prefix)\n",
    "    metric = None\n",
    "    if 'selected_text' in df:\n",
    "        true = df['selected_text']\n",
    "        metric = get_metric(true, pred)\n",
    "    return pred, metric\n",
    "\n",
    "################################### MODEL ###################################\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    if not params[\"label_smoothing\"]:\n",
    "        params[\"label_smoothing\"] = 0.\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=params[\"label_smoothing\"])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def build_model(opt):\n",
    "    ids = Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    # https://huggingface.co/transformers/model_doc/roberta.html?highlight=tfrobertamodel#tfrobertamodel\n",
    "    x = bert_model(ids_, attention_mask=att_, token_type_ids=tok_)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model2(opt):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "def pred_wrapper(model):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "\n",
    "    x = model([ids, att, tok])\n",
    "    x1_padded = tf.pad(x[0], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x[1], [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "\n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded, x2_padded])\n",
    "    return padded_model\n",
    "\n",
    "\n",
    "\n",
    "################################### CALLBACKS ###################################\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, model, word_ids, mask, segm_ids, start, end, df, tokenizer, n_fold, start_epoch, log_path, start_score, best_weights_path):\n",
    "        self.model = model\n",
    "\n",
    "        self.word_ids = word_ids\n",
    "        self.mask = mask\n",
    "        self.segm_ids = segm_ids\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.n_fold = n_fold\n",
    "        self.log_path = log_path\n",
    "        self.best_weights_path = best_weights_path\n",
    "\n",
    "        self.best = start_score\n",
    "        self.checkpoint = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Validation\n",
    "        pred_model = pred_wrapper(self.model)\n",
    "        start_proba, end_proba = tuple(pred_model.predict([self.word_ids, self.mask, self.segm_ids], verbose=1))\n",
    "        _, current = get_pred_and_metric(start_proba, end_proba, self.df, self.tokenizer)\n",
    "\n",
    "        # Save best model\n",
    "        if current > self.best:\n",
    "            self.best = current\n",
    "            self.model.save_weights(self.best_weights_path, overwrite=True)\n",
    "\n",
    "        # Log score info\n",
    "        abs_epoch = self.start_epoch + epoch\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(f'\\n[fold: {self.n_fold}, epoch: {abs_epoch}] Val Score : {current:.5f} (time: {(time.time() - self.checkpoint) // 60} min.)')\n",
    "        self.checkpoint = time.time()\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "\n",
    "    return 3e-5 * 0.2**epoch\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "################################### SEED & SESSION ###################################\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(params[\"seed\"])\n",
    "#     K.clear_session()\n",
    "#     config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=4)\n",
    "#     sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "#     tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "################################### ARGS ###################################\n",
    "\n",
    "if params[\"opt_name\"] == \"Adam\":\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "else:\n",
    "    assert False, \"unknown opt_name\"\n",
    "\n",
    "if params[\"lr_schedule_name\"] == \"default\":\n",
    "    lr_scheduler = lr_scheduler\n",
    "else:\n",
    "    assert False, \"unknown lr_schedule_name\"\n",
    "\n",
    "################################### LOGGING ###################################\n",
    "\n",
    "# define log-folder\n",
    "log_dir_path = \"../attempt_logs/{}\".format(params[\"att_num\"])\n",
    "if not os.path.exists(log_dir_path):\n",
    "    os.makedirs(log_dir_path)\n",
    "\n",
    "# define fold-folders log_path\n",
    "folder_path = \"{}/{}/\".format(log_dir_path, params[\"n_fold\"])\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# define & prefill log-file\n",
    "log_path = \"{}/log.txt\".format(log_dir_path, params[\"att_num\"])\n",
    "if not os.path.exists(log_path):\n",
    "    with open(log_path, 'w') as f:\n",
    "        for name, val in params.items():\n",
    "            f.write(\"{}: {} \\n\".format(name, val))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "################################### OPTIMIZATION PROCESSING ###################################\n",
    "\n",
    "# Fold Splitter\n",
    "skf = StratifiedKFold(n_splits=params[\"n_split\"], shuffle=True, random_state=777)\n",
    "splits = list(skf.split(input_ids, train_df.sentiment.values))\n",
    "\n",
    "# Splitting\n",
    "tr_idx, val_idx = splits[params[\"n_fold\"] - 1]\n",
    "\n",
    "train_df_ = train_df.loc[tr_idx].reset_index(drop=True)\n",
    "train_df_.index = tr_idx\n",
    "\n",
    "val_df = train_df.loc[val_idx].reset_index(drop=True)\n",
    "val_df.index = val_idx\n",
    "\n",
    "tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = input_ids[tr_idx,], attention_mask[tr_idx,], token_type_ids[tr_idx,], start_tokens[tr_idx,], end_tokens[tr_idx,]\n",
    "val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = input_ids[val_idx,], attention_mask[val_idx,], token_type_ids[val_idx,], start_tokens[val_idx,], end_tokens[val_idx,]\n",
    "\n",
    "\n",
    "#     N_TEST = 16 * 10\n",
    "#     tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = tr_word_ids[:N_TEST], tr_mask[:N_TEST], tr_segm_ids[:N_TEST], tr_starts[:N_TEST], tr_ends[:N_TEST]\n",
    "#     tr_idx = tr_idx[:N_TEST]\n",
    "#     train_df_ = train_df_[:N_TEST]\n",
    "\n",
    "# val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends\n",
    "# val_idx = tr_idx\n",
    "# val_df = train_df_\n",
    "\n",
    "# test_word_ids, test_mask, test_segm_ids = test_word_ids[16:32], test_mask[16:32], test_segm_ids[16:32]\n",
    "# test_df = test_df[16:32]\n",
    "\n",
    "\n",
    "print(f'##### FOLD {params[\"n_fold\"]} #####')\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Model Defining\n",
    "best_weights_path = \"{}/{}/best_model.h5\".format(log_dir_path, params[\"n_fold\"])\n",
    "if params[\"model_name\"] == \"default\":\n",
    "    model = build_model(opt)\n",
    "elif params[\"model_name\"] == \"v2.0\":\n",
    "    model = build_model2(opt)\n",
    "else:\n",
    "    assert False, \"unknown model_name param\"\n",
    "\n",
    "\n",
    "# Model Pretraining [optional]\n",
    "pre_trained_weights_path = \"../attempt_logs/{}/{}/best_model.h5\".format(\n",
    "    params[\"weights_att_num\"] or params[\"att_num\"],\n",
    "    params[\"n_fold\"]\n",
    ")\n",
    "pretrained_score = 0\n",
    "if os.path.exists(pre_trained_weights_path):\n",
    "    model.load_weights(pre_trained_weights_path)\n",
    "    padded_model = pred_wrapper(model)\n",
    "    start_proba, end_proba = tuple(padded_model.predict([val_word_ids, val_mask, val_segm_ids], verbose=1))\n",
    "    _, pretrained_score = get_pred_and_metric(start_proba, end_proba, val_df, tokenizer)\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(f'\\nWeights PreTrained from {pre_trained_weights_path}, pretrained_score: {pretrained_score:.5f}')\n",
    "\n",
    "# # Model Pretraining [optional, when we need only ensure total scores]\n",
    "# if not params[\"wo_fitting\"]:\n",
    "#     custom_callback = CustomCallback(\n",
    "#         model,\n",
    "#         val_word_ids, val_mask, val_segm_ids, val_starts, val_ends, val_df,\n",
    "#         tokenizer, \n",
    "#         params[\"n_fold\"],\n",
    "#         params[\"start_epoch\"],\n",
    "#         log_path,\n",
    "#         pretrained_score,\n",
    "#         best_weights_path\n",
    "#     )\n",
    "\n",
    "#     n_remain_epoch = params[\"n_epoch\"] - params[\"start_epoch\"] + 1\n",
    "#     model.fit(\n",
    "#         [tr_word_ids, tr_mask, tr_segm_ids], [tr_starts, tr_ends],\n",
    "#         batch_size=params[\"batch_size\"],\n",
    "#         epochs=n_remain_epoch,\n",
    "#         callbacks=[\n",
    "#             custom_callback,\n",
    "#             lr_scheduler\n",
    "#         ],\n",
    "#         verbose=1,\n",
    "#     )\n",
    "\n",
    "# model.load_weights(best_weights_path)\n",
    "# pred_model = pred_wrapper(model)\n",
    "\n",
    "\n",
    "# # train prediction\n",
    "# print(\"train prediction ...\")\n",
    "# train_start_proba, train_end_proba = tuple(pred_model.predict([tr_word_ids, tr_mask, tr_segm_ids], verbose=1))\n",
    "# _, train_metric = get_pred_and_metric(train_start_proba, train_end_proba, train_df_, tokenizer, out_prefix=\"{}/{}/train\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "# # validation prediction\n",
    "# print(\"validation prediction ...\")\n",
    "# val_start_proba, val_end_proba = tuple(pred_model.predict([val_word_ids, val_mask, val_segm_ids], verbose=1))\n",
    "# _, val_metric = get_pred_and_metric(val_start_proba, val_end_proba, val_df, tokenizer, out_prefix=\"{}/{}/validation\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "# # test prediction\n",
    "# print(\"test prediction ...\")\n",
    "# test_start_proba_, test_end_proba_ = tuple(pred_model.predict([test_word_ids, test_mask, test_segm_ids], verbose=1))\n",
    "# _ = get_pred_and_metric(test_start_proba_, test_end_proba_, test_df, tokenizer, out_prefix=\"{}/{}/test\".format(log_dir_path, params[\"n_fold\"]))\n",
    "\n",
    "# with open(log_path, 'a') as f:\n",
    "#     f.write(f'\\n[fold: {params[\"n_fold\"]}] Ensure Scores : train score: {train_metric:.5f}, validation score: {val_metric:.5f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def jaccard_expectation_loss(y_true, y_pred):\n",
    "    max_len = K.shape(y_true)[1] // 2\n",
    "    start_true, end_true = y_true[:, :max_len], y_true[:, max_len:]\n",
    "    start_pred, end_pred = y_pred[:, :max_len], y_pred[:, max_len:]\n",
    "    \n",
    "    # for true labels we can use argmax() function, cause labels don't involve in SGD\n",
    "    x_start = K.cast(K.argmax(start_true, axis=1), dtype=tf.float32)\n",
    "    x_end   = K.cast(K.argmax(end_true  , axis=1), dtype=tf.float32)\n",
    "    l = x_end - x_start + 1\n",
    "    \n",
    "    # some magic for getting indices matrix like this: [[0, 1, 2, 3], [0, 1, 2, 3]] \n",
    "    batch_size = K.shape(x_start)[0]\n",
    "    ind_row = tf.range(0, max_len, dtype=tf.float32)\n",
    "    ones_matrix = tf.ones([batch_size, max_len], dtype=tf.float32)\n",
    "    ind_matrix = ind_row * ones_matrix\n",
    "    \n",
    "    # expectations for x_start^* (x_start_pred) and x_end^* (x_end_pred)\n",
    "    x_start_pred = K.sum(start_pred * ind_matrix, axis=1)\n",
    "    x_end_pred   = K.sum(end_pred   * ind_matrix, axis=1)\n",
    "    \n",
    "    relu11 = K.relu(x_start_pred - x_start)\n",
    "    relu12 = K.relu(x_end   - x_end_pred  )\n",
    "    relu21 = K.relu(x_start - x_start_pred)\n",
    "    relu22 = K.relu(x_end_pred   - x_end  )\n",
    "    \n",
    "    intersection = l - relu11 - relu12\n",
    "    union = l + relu21 + relu22\n",
    "    jel = intersection / union\n",
    "    \n",
    "    return 1 - jel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(opt):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "    \n",
    "    y = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=y)\n",
    "#     model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    model.compile(loss=jaccard_expectation_loss, optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Equal_2 (TensorFlow [(None, 96)]         0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast_2 (TensorFlowO [(None, 96)]         0           tf_op_layer_Equal_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None,)]            0           tf_op_layer_Cast_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None,)]            0           tf_op_layer_Sum_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Max_2 (TensorFlowOp [()]                 0           tf_op_layer_Sub_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6/Pac [(2,)]               0           tf_op_layer_Max_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7/Pac [(2,)]               0           tf_op_layer_Max_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8/Pac [(2,)]               0           tf_op_layer_Max_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, None)]       0           input_7[0][0]                    \n",
      "                                                                 tf_op_layer_strided_slice_6/Pack[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(None, None)]       0           input_8[0][0]                    \n",
      "                                                                 tf_op_layer_strided_slice_7/Pack[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, None)]       0           input_9[0][0]                    \n",
      "                                                                 tf_op_layer_strided_slice_8/Pack[\n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_2 (TFRobertaMo ((None, None, 768),  124645632   tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)           (None, None, 768)    0           tf_roberta_model_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)           (None, None, 768)    0           tf_roberta_model_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 768)    1180416     dropout_118[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 768)    1180416     dropout_119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, 768)    0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, 768)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 1)      769         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 1)      769         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, None)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, None)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None)         0           activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 127,008,002\n",
      "Trainable params: 127,008,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model = build_model2(opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Reduction axis 1 is empty in shape [8,0]\n\t [[node jaccard_expectation_loss/ArgMax_1 (defined at <ipython-input-8-60e2b736ce17>:10) ]]\n\t [[gradient_tape/model_2/tf_roberta_model_2/roberta/embeddings/position_embeddings/embedding_lookup/Reshape_1/_46]]\n  (1) Invalid argument:  Reduction axis 1 is empty in shape [8,0]\n\t [[node jaccard_expectation_loss/ArgMax_1 (defined at <ipython-input-8-60e2b736ce17>:10) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_49086]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node jaccard_expectation_loss/ArgMax_1:\n jaccard_expectation_loss/strided_slice_2 (defined at <ipython-input-8-60e2b736ce17>:5)\n\nInput Source operations connected to node jaccard_expectation_loss/ArgMax_1:\n jaccard_expectation_loss/strided_slice_2 (defined at <ipython-input-8-60e2b736ce17>:5)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cdc9715b87cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Reduction axis 1 is empty in shape [8,0]\n\t [[node jaccard_expectation_loss/ArgMax_1 (defined at <ipython-input-8-60e2b736ce17>:10) ]]\n\t [[gradient_tape/model_2/tf_roberta_model_2/roberta/embeddings/position_embeddings/embedding_lookup/Reshape_1/_46]]\n  (1) Invalid argument:  Reduction axis 1 is empty in shape [8,0]\n\t [[node jaccard_expectation_loss/ArgMax_1 (defined at <ipython-input-8-60e2b736ce17>:10) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_49086]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node jaccard_expectation_loss/ArgMax_1:\n jaccard_expectation_loss/strided_slice_2 (defined at <ipython-input-8-60e2b736ce17>:5)\n\nInput Source operations connected to node jaccard_expectation_loss/ArgMax_1:\n jaccard_expectation_loss/strided_slice_2 (defined at <ipython-input-8-60e2b736ce17>:5)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "y_true = np.concatenate([tr_starts, tr_ends], axis=1)\n",
    "model.fit(\n",
    "    [tr_word_ids, tr_mask, tr_segm_ids], y_true,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
