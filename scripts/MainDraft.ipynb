{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from constants import *\n",
    "from callback import CustomCallback\n",
    "from base_model import get_base_model\n",
    "from head_model import get_combined_model\n",
    "from loss import get_loss\n",
    "from data_reading import read_data\n",
    "from data_preparation import get_train_data, get_test_data\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12516478170147904572\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13004211799845641620\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7940792892885777162\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3868065792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3968659607204203221\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_path\": \"..\",\n",
    "    \"mode\": \"debug:1600\",\n",
    "    \"att_num\": 69,\n",
    "    \"weights_att_num\": None,\n",
    "    \"n_fold\": 4,\n",
    "    \"start_epoch\": 4,\n",
    "    \"wo_fitting\": False,\n",
    "\n",
    "    \"base_model\": \"roberta-base\",# \"roberta-base\",\n",
    "    \"head_model\": \"default\",\n",
    "    \"lr\": 0.00003, # \"base_lr\": 0.00002, \"max_lr\": 0.00004,\n",
    "    \"loss\": \"CCE\", #\"{'CCE':1, 'JEL':0.1}\",\n",
    "    \"label_smoothing\": 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data...\n",
      "Build & Compile model...\n",
      "roberta-base\n",
      "Prepare data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e0fa510e41499bbf62cb26495601e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe248d08f7e34d23bd0de1e1be771b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data...\n",
      "Check Correcness...\n",
      "correct samples: 0.883750\n",
      "preprocessing OK!\n",
      "##### FOLD 4 #####\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "50/50 [==============================] - 13s 261ms/step\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 5.1663 - activation_15_loss: 2.5582 - activation_16_loss: 2.6081 - lr: 3.0000e-05\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 14s 271ms/step\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 4.5635 - activation_15_loss: 2.2853 - activation_16_loss: 2.2781 - lr: 6.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Seed $ Logging\n",
    "seed_everything(SEED)\n",
    "log_dir_path, log_path = init_logging(params)\n",
    "\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f'\\n[base_model] {params[\"base_model\"]}')\n",
    "\n",
    "# Read  data\n",
    "print(\"Read data...\")\n",
    "train_df, test_df, submission_df = read_data(params)\n",
    "\n",
    "# Splitter\n",
    "skf = StratifiedKFold(n_splits=N_SPLIT, shuffle=True, random_state=777)\n",
    "splits = list(skf.split(train_df.index.values, train_df.sentiment.values))\n",
    "tr_idx, val_idx = splits[params[\"n_fold\"] - 1]\n",
    "test_idx = np.arange(N_TEST)\n",
    "\n",
    "if \"debug\" in params[\"mode\"]:\n",
    "    n_debug = int(params[\"mode\"].split(\":\")[1])\n",
    "    tr_idx, val_idx = tr_idx[:n_debug], val_idx[:n_debug]\n",
    "    test_idx = test_idx[:n_debug]\n",
    "\n",
    "# Build & Compile model\n",
    "print(\"Build & Compile model...\")\n",
    "tokenizer, base_model = get_base_model(params)\n",
    "combined_model = get_combined_model(base_model, params)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "loss = get_loss(params)\n",
    "combined_model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "# Prepare  data\n",
    "print(\"Prepare data...\")\n",
    "known_idx = np.array(list(set(tr_idx) | set(val_idx)))\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens, train_sample_ind2new_ind2old_ind = get_train_data(train_df, tokenizer, idx=known_idx)\n",
    "test_word_ids, test_mask, test_segm_ids, test_sample_ind2new_ind2old_ind = get_test_data(test_df, tokenizer, idx=test_idx)\n",
    "\n",
    "# # Model hash\n",
    "# print(f'base_model hash: {np.array(base_model(test_word_ids[:16], test_mask[:16], test_segm_ids[:16])[0]).sum():.3}')\n",
    "# print(f'head_model hash: {combined_model.layers[-6].weights[0].numpy().sum():.3}')\n",
    "\n",
    "# Splitting data\n",
    "print(\"Splitting data...\")\n",
    "tr_df = train_df.loc[tr_idx].reset_index(drop=True).set_index(tr_idx)\n",
    "val_df = train_df.loc[val_idx].reset_index(drop=True).set_index(val_idx)\n",
    "\n",
    "tr_word_ids, tr_mask, tr_segm_ids, tr_starts, tr_ends = input_ids[tr_idx,], attention_mask[tr_idx,], token_type_ids[tr_idx,], start_tokens[tr_idx,], end_tokens[tr_idx,]\n",
    "tr_targets = np.concatenate([tr_starts, tr_ends], axis=1)\n",
    "val_word_ids, val_mask, val_segm_ids, val_starts, val_ends = input_ids[val_idx,], attention_mask[val_idx,], token_type_ids[val_idx,], start_tokens[val_idx,], end_tokens[val_idx,]\n",
    "\n",
    "# Check Correcness\n",
    "print(\"Check Correcness...\")\n",
    "tr_df[\"is_correct\"] = tr_df.apply(lambda row: (\" \" + row.text + \" \").find(\" \" + row.selected_text + \" \") >= 0, axis=1)\n",
    "print(f'correct samples: {tr_df[\"is_correct\"].mean():3f}')\n",
    "\n",
    "tr_df[\"recover_selected_text\"] = get_st_prediction(tr_starts, tr_ends, tr_df, train_sample_ind2new_ind2old_ind)\n",
    "tr_df[\"recover_jaccard\"] = tr_df.apply(lambda row: jaccard(row[\"recover_selected_text\"], row[\"selected_text\"]), axis=1)\n",
    "assert np.all(tr_df[tr_df[\"is_correct\"]][\"recover_jaccard\"] == 1)\n",
    "print(f'preprocessing OK!')\n",
    "\n",
    "\n",
    "print(f'##### FOLD {params[\"n_fold\"]} #####')\n",
    "gc.collect()\n",
    "\n",
    "# Model Paths & Pretraining (optional)\n",
    "best_weights_path = f'{log_dir_path}/{params[\"n_fold\"]}/best_model.h5'\n",
    "pre_trained_weights_path = f'../attempt_logs/{params[\"weights_att_num\"] or params[\"att_num\"]}/{params[\"n_fold\"]}/best_model.h5'\n",
    "\n",
    "pretrained_score = 0\n",
    "# if os.path.exists(pre_trained_weights_path):\n",
    "#     combined_model.load_weights(pre_trained_weights_path)\n",
    "#     start_proba, end_proba = get_proba_prediction(combined_model, val_word_ids, val_mask, val_segm_ids)\n",
    "#     pretrained_score = get_score(start_proba, end_proba, val_df, train_sample_ind2new_ind2old_ind)\n",
    "#     with open(log_path, 'a') as f:\n",
    "#         f.write(f'\\nWeights PreTrained from {pre_trained_weights_path}, pretrained_score: {pretrained_score:.5f}')\n",
    "\n",
    "# Training (optional)\n",
    "if not params[\"wo_fitting\"]:\n",
    "    lr_scheduler = LearningRateScheduler(lambda epoch: 3e-5 * 0.2**epoch)\n",
    "    custom_callback = CustomCallback(\n",
    "        combined_model,\n",
    "        val_word_ids, val_mask, val_segm_ids, val_df, train_sample_ind2new_ind2old_ind,\n",
    "        params[\"n_fold\"],\n",
    "        params[\"start_epoch\"],\n",
    "        log_path,\n",
    "        pretrained_score,\n",
    "        best_weights_path\n",
    "    )\n",
    "\n",
    "    n_epoch = N_EPOCH - params[\"start_epoch\"] + 1\n",
    "    combined_model.fit(\n",
    "        [tr_word_ids, tr_mask, tr_segm_ids], [tr_starts, tr_ends], #tr_targets,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=n_epoch,\n",
    "        callbacks=[\n",
    "            custom_callback,\n",
    "            lr_scheduler\n",
    "        ],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "combined_model.load_weights(best_weights_path)\n",
    "\n",
    "# scores = {}\n",
    "# for name, word_ids, mask, segm_ids, df, sample_ind2new_ind2old_ind in [\n",
    "#     (\"train\"      ,  tr_word_ids,   tr_mask,   tr_segm_ids,   tr_df, train_sample_ind2new_ind2old_ind),\n",
    "#     (\"validation\" , val_word_ids,  val_mask,  val_segm_ids,  val_df, train_sample_ind2new_ind2old_ind),\n",
    "#     (\"test\"      , test_word_ids, test_mask, test_segm_ids, test_df,  test_sample_ind2new_ind2old_ind)\n",
    "# ]:\n",
    "#     print(f'{name} prediction ...')\n",
    "#     start_proba, end_proba = get_proba_prediction(combined_model, word_ids, mask, segm_ids)\n",
    "#     if name != \"test\":\n",
    "#         scores[name] = get_score(start_proba, end_proba, df, sample_ind2new_ind2old_ind)\n",
    "\n",
    "# with open(log_path, 'a') as f:\n",
    "#     f.write(f'\\n[fold: {params[\"n_fold\"]}] Ensure Scores : train score: {scores[\"train\"]:.5f}, validation score: {scores[\"validation\"]:.5f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.keras.layers.Dropout(0.1)(x)\n",
    "x1 = tf.keras.layers.Conv1D(, 2, padding='same')(x1)\n",
    "x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "x1 = tf.keras.layers.Dense(1)(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "x1 = tf.keras.layers.Activation('softmax')(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_arr1=[128, 64],\n",
    "kernel_size_arr1=[2, 2],\n",
    "dropout_arr1=[0.1, 0.1],\n",
    "batch_norm_arr1=[False, False],\n",
    "act_name_arr1=['leaky_relu', 'leaky_relu'],\n",
    "res_mode_arr1=[False, False],\n",
    "kernel_mode_arr1=[None, None],\n",
    "act_mode_arr1=[None, None],\n",
    "dropout_arr2=[0.1],\n",
    "batch_norm_arr2=[False],\n",
    "dense_units_arr2=[1],\n",
    "act_name_arr2=[None],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model_params = {\n",
    "    \"filters_arr1\":[768],\n",
    "    \"kernel_size_arr1\": [2],\n",
    "    \"dropout_arr1\": [0.1],\n",
    "    \"batch_norm_arr1\": [False],\n",
    "    \"act_name_arr1\": ['leaky_relu'],\n",
    "    \"res_mode_arr1\": [False],\n",
    "    \"kernel_mode_arr1\": [None],\n",
    "    \"act_mode_arr1\": [None],\n",
    "    \"dropout_arr2\": [None],\n",
    "    \"batch_norm_arr2\": [False],\n",
    "    \"dense_units_arr2\": [1],\n",
    "    \"act_name_arr2\": [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = str(head_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.replace(\" \", \"\").replace(\":\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'filters_arr1':[768],'kernel_size_arr1':[2],'dropout_arr1':[0.1],'batch_norm_arr1':[False],'act_name_arr1':['leaky_relu'],'res_mode_arr1':[False],'kernel_mode_arr1':[None],'act_mode_arr1':[None],'dropout_arr2':[None],'batch_norm_arr2':[False],'dense_units_arr2':[1],'act_name_arr2':[None]}\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.replace(\"-\", \":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'filters_arr1':[768],'kernel_size_arr1':[2],'dropout_arr1':[0.1],'batch_norm_arr1':[False],'act_name_arr1':['leaky_relu'],'res_mode_arr1':[False],'kernel_mode_arr1':[None],'act_mode_arr1':[None],'dropout_arr2':[None],'batch_norm_arr2':[False],'dense_units_arr2':[1],'act_name_arr2':[None]}\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filters_arr1': [768],\n",
       " 'kernel_size_arr1': [2],\n",
       " 'dropout_arr1': [0.1],\n",
       " 'batch_norm_arr1': [False],\n",
       " 'act_name_arr1': ['leaky_relu'],\n",
       " 'res_mode_arr1': [False],\n",
       " 'kernel_mode_arr1': [None],\n",
       " 'act_mode_arr1': [None],\n",
       " 'dropout_arr2': [None],\n",
       " 'batch_norm_arr2': [False],\n",
       " 'dense_units_arr2': [1],\n",
       " 'act_name_arr2': [None]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model_params = {\n",
    "    \"filters_arr1\": [128, 64],\n",
    "    \"kernel_size_arr1\": [2, 2],\n",
    "    \"dropout_arr1\": [0.1, 0.1],\n",
    "    \"batch_norm_arr1\": [False, False],\n",
    "    \"act_name_arr1\": ['leaky_relu', 'leaky_relu'],\n",
    "    \"res_mode_arr1\": [False, False],\n",
    "    \"kernel_mode_arr1\": [None, None],\n",
    "    \"act_mode_arr1\": [None, None],\n",
    "    \"dropout_arr2\": [0.1],\n",
    "    \"batch_norm_arr2\": [False],\n",
    "    \"dense_units_arr2\": [1],\n",
    "    \"act_name_arr2\": [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6526419999999999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.65487 + 0.66263 + 0.65088 + 0.64941 + 0.64542) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fold: 1, epoch: 5] Val Score : 0.65487 (time: 2.0 min.)\n",
    "[fold: 2, epoch: 5] Val Score : 0.66263 (time: 2.0 min.)\n",
    "[fold: 3, epoch: 5] Val Score : 0.65088 (time: 2.0 min.)\n",
    "[fold: 4, epoch: 5] Val Score : 0.64941 (time: 2.0 min.)\n",
    "[fold: 5, epoch: 5] Val Score : 0.64542 (time: 2.0 min.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import models\n",
    "tf.compat.v1.disable_eager_execution() # for tf placeholders\n",
    "\n",
    "def smoothed_cce_loss(y_true, y_pred):\n",
    "    print(\"y_true\", y_true)\n",
    "    print(\"y_pred\", y_pred)\n",
    "#     ls = params[\"label_smoothing\"] or 0\n",
    "    ls = 0.1\n",
    "\n",
    "#     max_len = K.shape(y_pred)[1] // 2\n",
    "    max_len = 100\n",
    "    start_pred, end_pred = y_pred[:, :max_len], y_pred[:, MAX_LEN:MAX_LEN + max_len]\n",
    "    start_true, end_true = y_true[:, :max_len], y_true[:, MAX_LEN:MAX_LEN + max_len]\n",
    "    print(\"start_pred\", start_pred)\n",
    "    print(\"end_pred\", end_pred)\n",
    "    print(\"start_true\", start_true)\n",
    "    print(\"end_true\", end_true)\n",
    "    \n",
    "\n",
    "    start_loss = tf.keras.losses.categorical_crossentropy(start_true, start_pred, label_smoothing=ls)\n",
    "    end_loss =   tf.keras.losses.categorical_crossentropy(  end_true,   end_pred, label_smoothing=ls)\n",
    "    loss = tf.reduce_mean(start_loss + end_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 226), (8, 226))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 113\n",
    "\n",
    "y_true = tr_targets[:8]\n",
    "y_pred = tr_targets[:8]\n",
    "\n",
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 13), (8, 13))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 100\n",
    "\n",
    "assert y_pred[::, max_len:MAX_LEN].sum() + y_pred[::, MAX_LEN + max_len:].sum() == 0\n",
    "y_pred[::, max_len:MAX_LEN].shape, y_pred[::, MAX_LEN + max_len:].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true Tensor(\"Placeholder_26:0\", shape=(None, 226), dtype=float32)\n",
      "y_pred Tensor(\"Placeholder_27:0\", shape=(None, 226), dtype=float32)\n",
      "start_pred Tensor(\"strided_slice_84:0\", shape=(None, 100), dtype=float32)\n",
      "end_pred Tensor(\"strided_slice_85:0\", shape=(None, 100), dtype=float32)\n",
      "start_true Tensor(\"strided_slice_86:0\", shape=(None, 100), dtype=float32)\n",
      "end_true Tensor(\"strided_slice_87:0\", shape=(None, 100), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1913834"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 226])\n",
    "y_true_inp = tf.compat.v1.placeholder(tf.float32, shape=[None, 226])\n",
    "\n",
    "cce = smoothed_cce_loss(y_pred_inp, y_true_inp)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "jel = sess.run(cce, feed_dict={y_pred_inp: y_pred, y_true_inp: y_true})\n",
    "jel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 226)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model hash: 2.49e+04\n",
    "head_model hash: -0.233"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
